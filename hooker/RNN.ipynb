{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(base_dir+'train_1.csv', nrows=100)\n",
    "dates = [c for c in full_df.columns if c !='Page']\n",
    "val_dates = dates[-61:]\n",
    "val = full_df[['Page']+val_dates]\n",
    "train = full_df.drop(val_dates, axis=1)\n",
    "filled_train = train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = filled_train.drop('Page', axis=1).values\n",
    "y = val.fillna(0).drop('Page', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 489) (100, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocessing\n",
    " * scale each time series to have 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 489)\n",
      "(100, 489, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.010761664611856"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = StandardScaler()\n",
    "# discussion on which scaler to use here: https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/38274\n",
    "# each ts should have 0 mean and unit variance\n",
    "# since the time series are the 'features' being scaled, transpose first\n",
    "X = scaler.fit_transform(X.T).T\n",
    "print(X.shape)\n",
    "assert(np.isclose(np.mean(X[0]),0))\n",
    "# input shape: samples, timesteps, features\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(X.shape)\n",
    "np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_SEED = 123456\n",
    "n_pred = 60 # number of values to predict\n",
    "n_cond = 60 # number of values to condition on\n",
    "\n",
    "class DataProvider(object):\n",
    "    \"\"\"Generic data provider.\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, targets, batch_size, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None):\n",
    "        \"\"\"Create a new data provider object.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Array of data input features of shape\n",
    "                (num_data, input_dim).\n",
    "            targets (ndarray): Array of data output targets of shape\n",
    "                (num_data, output_dim) or (num_data,) if output_dim == 1.\n",
    "            batch_size (int): Number of data points to include in each batch.\n",
    "            max_num_batches (int): Maximum number of batches to iterate over\n",
    "                in an epoch. If `max_num_batches * batch_size > num_data` then\n",
    "                only as many batches as the data can be split into will be\n",
    "                used. If set to -1 all of the data will be used.\n",
    "            shuffle_order (bool): Whether to randomly permute the order of\n",
    "                the data before each epoch.\n",
    "            rng (RandomState): A seeded random number generator.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        if batch_size < 1:\n",
    "            raise ValueError('batch_size must be >= 1')\n",
    "        self._batch_size = batch_size\n",
    "        if max_num_batches == 0 or max_num_batches < -1:\n",
    "            raise ValueError('max_num_batches must be -1 or > 0')\n",
    "        self._max_num_batches = max_num_batches\n",
    "        self._update_num_batches()\n",
    "        self.shuffle_order = shuffle_order\n",
    "        self._current_order = np.arange(inputs.shape[0])\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(DEFAULT_SEED)\n",
    "        self.rng = rng\n",
    "        self.new_epoch()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"Number of data points to include in each batch.\"\"\"\n",
    "        return self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, value):\n",
    "        if value < 1:\n",
    "            raise ValueError('batch_size must be >= 1')\n",
    "        self._batch_size = value\n",
    "        self._update_num_batches()\n",
    "\n",
    "    @property\n",
    "    def max_num_batches(self):\n",
    "        \"\"\"Maximum number of batches to iterate over in an epoch.\"\"\"\n",
    "        return self._max_num_batches\n",
    "\n",
    "    @max_num_batches.setter\n",
    "    def max_num_batches(self, value):\n",
    "        if value == 0 or value < -1:\n",
    "            raise ValueError('max_num_batches must be -1 or > 0')\n",
    "        self._max_num_batches = value\n",
    "        self._update_num_batches()\n",
    "\n",
    "    def _update_num_batches(self):\n",
    "        \"\"\"Updates number of batches to iterate over.\"\"\"\n",
    "        # maximum possible number of batches is equal to number of whole times\n",
    "        # batch_size divides in to the number of data points which can be\n",
    "        # found using integer division\n",
    "        possible_num_batches = self.inputs.shape[0] // self.batch_size\n",
    "        if self.max_num_batches == -1:\n",
    "            self.num_batches = possible_num_batches\n",
    "        else:\n",
    "            self.num_batches = min(self.max_num_batches, possible_num_batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Implements Python iterator interface.\n",
    "\n",
    "        This should return an object implementing a `next` method which steps\n",
    "        through a sequence returning one element at a time and raising\n",
    "        `StopIteration` when at the end of the sequence. Here the object\n",
    "        returned is the DataProvider itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def new_epoch(self):\n",
    "        \"\"\"Starts a new epoch (pass through data), possibly shuffling first.\"\"\"\n",
    "        self._curr_batch = 0\n",
    "        if self.shuffle_order:\n",
    "            self.shuffle()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the provider to the initial state.\"\"\"\n",
    "        inv_perm = np.argsort(self._current_order)\n",
    "        self._current_order = self._current_order[inv_perm]\n",
    "        self.inputs = self.inputs[inv_perm]\n",
    "        self.targets = self.targets[inv_perm]\n",
    "        self.new_epoch()\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Randomly shuffles order of data.\"\"\"\n",
    "        perm = self.rng.permutation(self.inputs.shape[0])\n",
    "        self._current_order = self._current_order[perm]\n",
    "        self.inputs = self.inputs[perm]\n",
    "        self.targets = self.targets[perm]\n",
    "        if hasattr(self, 'track_ids'):\n",
    "            self.track_ids = self.track_ids[perm]\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Returns next data batch or raises `StopIteration` if at end.\"\"\"\n",
    "        if self._curr_batch + 1 > self.num_batches:\n",
    "            # no more batches in current iteration through data set so start\n",
    "            # new epoch ready for another pass and indicate iteration is at end\n",
    "            self.new_epoch()\n",
    "            raise StopIteration()\n",
    "        # create an index slice corresponding to current batch number\n",
    "        batch_slice = slice(self._curr_batch * self.batch_size,\n",
    "                            (self._curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self._curr_batch += 1\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "    # Python 3.x compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "class MultiTSDataProvider(DataProvider):\n",
    "    \n",
    "    def __init__(self, which_set='train', batch_size=100, max_num_batches=-1,\n",
    "                 shuffle_order=True, rng=None, n_pred=60, n_cond=60, stride_length=10,\n",
    "                 scaler=StandardScaler(), base_dir = '../data/', n_ts=100):\n",
    "        self.n_pred = n_pred # number of values to predict\n",
    "        self.n_cond = n_cond # number of values on which to condition predictions\n",
    "        full_df = pd.read_csv(base_dir+'train_1.csv', nrows=n_ts)\n",
    "        dates = [c for c in full_df.columns if c !='Page']\n",
    "        val_dates = dates[-n_pred:]\n",
    "        self.val_dates = val_dates\n",
    "        if which_set == 'train':\n",
    "            inputs = full_df.drop(['Page'] + val_dates, axis=1).fillna(0).values\n",
    "        elif which_set == 'val':\n",
    "            inputs = full_df[val_dates].fillna(0).values\n",
    "          # each ts should have 0 mean and unit variance\n",
    "        if scaler:\n",
    "            inputs = scaler.fit_transform(inputs)\n",
    "            print(np.max(inputs))\n",
    "        print(inputs.shape)\n",
    "        \n",
    "        window_length = n_cond + n_pred\n",
    "        n_windows = int(np.floor(np.divide(inputs.shape[1] - window_length,\n",
    "                                           stride_length) + 1))\n",
    "        start_index = 0\n",
    "        window_array = np.ndarray((inputs.shape[0], n_windows, window_length))\n",
    "        for i in range(n_windows):\n",
    "            window_array[:,i,:] = inputs[:, start_index:start_index+window_length]\n",
    "            start_index += stride_length\n",
    "        print(window_array.shape)\n",
    "        window_array = window_array.reshape((-1,window_length,1))\n",
    "        print(window_array.shape)\n",
    "        print('{} overlapping windows of length {} in training date range'.format(n_windows, window_length))\n",
    "#         inputs = w.reshape((-1, window_length, inputs.shape[-1])) # reshape inputs to (n_sample, length, n_features)\n",
    "        inputs = np.pad(window_array[:,:-1,:],((0,0), (1,0), (0,0)),mode='constant')\n",
    "        targets = inputs[:,self.n_cond:,:]\n",
    "#         inputs, targets = inputs[:,:self.n_cond,:], inputs[:,self.n_cond:,:]\n",
    "        inputs = inputs.reshape((inputs.shape[0], -1))\n",
    "        targets = targets.reshape((targets.shape[0], -1))\n",
    "        print(inputs.shape, targets.shape)\n",
    "        super(MultiTSDataProvider, self).__init__(\n",
    "            inputs, targets, batch_size, max_num_batches, shuffle_order, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.86570145996\n",
      "(100, 490)\n",
      "(100, 38, 120)\n",
      "(3800, 120, 1)\n",
      "38 overlapping windows of length 120 in training date range\n",
      "(3800, 120) (3800, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.5193152455075101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MultiTSDataProvider(scaler=StandardScaler())\n",
    "a = train_data.next()[0]\n",
    "np.max(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function:\n",
    "\n",
    "Modified SMAPE. Note that using MAE gives not much different results, but easier and faster to train, so I recommend MAE for starters. (https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/38337)\n",
    "\n",
    "### Output:\n",
    "\n",
    "You can predict one day, refit the model with previous days + predicted day, predict next day, etc, but it would be too slow and inefficient. The real power of RNN's is that you can build generative model, and predict all 60 days at once. (https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/38337)\n",
    "\n",
    "ONLY PREDICT VALUES FOR A SINGLE TIME SERIES AT ONCE (c.f. the neural network eqn. 1 which outputs hidden rnn states for a single time series at a time, which are then combined to form the joint likelihood above) - DIFFERENT TIME SERIES ARE USED TO GENERATE DIFFERENT TRAINING EXAMPLES; SINGLE MODEL IS TRAINED ON ALL OF THEM.\n",
    "\n",
    "a substantial amount\n",
    "of data on past behavior of similar, related time series can be leveraged for making a forecast for an\n",
    "individual time series. Using data from related time series not only allows fitting more complex (and\n",
    "hence potentially more accurate) models without overfitting\n",
    "\n",
    "https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "\n",
    "Simplest model is probably some kind of ar\n",
    "\n",
    "### Training examples\n",
    "\n",
    "Secondly, due to the imbalance in the data, a stochastic optimization procedure that picks training\n",
    "instances uniformly at random will visit the small number time series with a large scale very infrequently,\n",
    "which result in underfitting those time series. This could be especially problematic in the\n",
    "demand forecasting setting, where high-velocity items can exhibit qualitatively different behavior\n",
    "than low-velocity items, and having an accurate forecast for high-velocity items might be more important\n",
    "for meeting certain business objectives. To counteract this effect, we sample the examples\n",
    "non-uniformly during training. In particular, in our weighted sampling scheme, the probability of\n",
    "selecting a window from an example with scale νi is proportional to νi.\n",
    "\n",
    "### Tensorflow seq2seq\n",
    "[github seq2seq helpers](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py)\n",
    "[ex. model built using above](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py)\n",
    "[keras lib](https://github.com/farizrahman4u/seq2seq)\n",
    "[new api (1.3) seq2seq](https://medium.com/@ilblackdragon/tensorflow-sequence-to-sequence-3d9d2e238084)\n",
    "[seq2seq expts](https://github.com/raindeer/seq2seq_experiments/blob/master/model.py#L81)\n",
    "* use tied_rnn_seq2seq if using tf\n",
    "* will need to write custom loss func\n",
    "* not sure the keras library will work because although we are sort of using encoder-decoder, we're not really outputting a 'context vector', just transferring the hidden state\n",
    "* to pass predicted values in as inputs when decoding: https://stackoverflow.com/questions/38050333/how-to-predict-a-simple-sequence-using-seq2seq-from-tensorflow (although there is an option just to use the true values during training, which is what the amazon paper does).\n",
    "\n",
    "The initial state\n",
    "of the encoder hi,0 as well as zi,0 are initialized to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 100\n",
    "n_cond = 60\n",
    "n_pred = 60\n",
    "\n",
    "def compute_next_input(output, i):\n",
    "    p = tf.reshape(tf.matmul(output, weights), (-1,)) + bias\n",
    "    return tf.reshape(p,(-1,1))\n",
    "\n",
    "looper = lambda output, i: tf.reshape(tf.reshape(tf.matmul(output, weights), (-1,)) + bias, (-1,1))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    for i in range(n_cond):\n",
    "        # individual inputs are single numbers \n",
    "        encoder_inputs.append(tf.placeholder(tf.float32, shape=[None,1], name=\"encoder{0}\".format(i)))\n",
    "    for i in range(n_pred):\n",
    "        decoder_inputs.append(tf.placeholder(tf.float32, shape=[None,1], name=\"decoder{0}\".format(i)))\n",
    "    for i in range(n_pred):\n",
    "        targets.append(tf.placeholder(tf.float32, shape=[None], name=\"target{0}\".format(i)))\n",
    "        \n",
    "#     targets = [decoder_inputs[i+1] for i in range(len(decoder_inputs)-1)] # targets are next inputs\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.tied_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "    # to do val, need to define a loop function\n",
    "    # outputs is a list where each thing has shape (batch_size,n_lstm)\n",
    "    # loop_function will need to apply a linear transform\n",
    "    preds = []\n",
    "    losses = []\n",
    "    for output, target in zip(outputs, targets):\n",
    "        bias = tf.Variable(initial_value=1.0)\n",
    "        weights = tf.Variable(tf.truncated_normal(\n",
    "            [lstm_size, 1], stddev=2. / (lstm_size + 1)**0.5), \n",
    "        'weights')\n",
    "#         weights = tf.Variable(tf.ones([lstm_size,1]))\n",
    "        pred = tf.reshape(tf.matmul(output, weights), (-1,)) + bias\n",
    "        preds.append(pred) # each pred is shape [batch_size]\n",
    "        losses.append(tf.reduce_mean(tf.abs(pred-target))) # error for a given output position averaged over the batch\n",
    "    \n",
    "    mae = tf.reduce_mean(losses) # compute error averaged over the timesteps (and batch samples)\n",
    "#     mae = tf.metrics.mean_absolute_error(targets, outputs)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(mae)\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5777953919\n",
      "(1000, 490)\n",
      "(1000, 38, 120)\n",
      "(38000, 120, 1)\n",
      "38 overlapping windows of length 120 in training date range\n",
      "(38000, 120) (38000, 60)\n",
      "End of epoch 1: running error average = 0.064\n",
      " mean error average = 0.055\n",
      "End of epoch 2: running error average = 0.025\n",
      " mean error average = 0.055\n",
      "End of epoch 3: running error average = 0.016\n",
      " mean error average = 0.055\n",
      "End of epoch 4: running error average = 0.013\n",
      " mean error average = 0.055\n",
      "End of epoch 5: running error average = 0.011\n",
      " mean error average = 0.055\n",
      "End of epoch 6: running error average = 0.010\n",
      " mean error average = 0.055\n",
      "End of epoch 7: running error average = 0.009\n",
      " mean error average = 0.055\n",
      "End of epoch 8: running error average = 0.008\n",
      " mean error average = 0.055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4bb274ea7940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0merrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmean_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_cond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexhooker/projects/informatics/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexhooker/projects/informatics/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexhooker/projects/informatics/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexhooker/projects/informatics/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexhooker/projects/informatics/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = MultiTSDataProvider(n_ts=1000)\n",
    "\n",
    "sess = tf.Session(graph=graph)\n",
    "sess.run(init_op)\n",
    "errs = []\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_error=0.\n",
    "    mean_running_error = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "    #     print(input_batch.shape, target_batch.shape)\n",
    "        feed_dict = {}\n",
    "        for i in range(n_cond):\n",
    "            feed_dict[encoder_inputs[i].name] = input_batch[:,i].reshape(-1,1)\n",
    "        for i in range(n_pred):\n",
    "            feed_dict[decoder_inputs[i].name] = input_batch[:,n_cond+i].reshape(-1,1)\n",
    "        for i in range(n_pred):\n",
    "            feed_dict[targets[i].name] = target_batch[:,i]\n",
    "        _, err, predictions = sess.run([train_step, mae, preds], feed_dict=feed_dict)\n",
    "        errs.append(err)\n",
    "        mean_preds = np.mean(input_batch[:,:n_cond], axis=1).reshape(-1,1)\n",
    "#         print(mean_preds.shape, target_batch.shape)\n",
    "        mean_errs = np.abs(mean_preds - target_batch)\n",
    "#         print(mean_errs.shape)\n",
    "        batch_mean_err = np.mean(mean_errs)\n",
    "        running_error += err\n",
    "        mean_running_error += batch_mean_err\n",
    "    running_error /= train_data.num_batches\n",
    "    mean_running_error /= train_data.num_batches\n",
    "    print('End of epoch {0}: running error average = {1:.3f}\\n mean error average = {2:.3f}'.format(e + 1, running_error, mean_running_error))\n",
    "# print(len(preds), preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.043 / 0.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
