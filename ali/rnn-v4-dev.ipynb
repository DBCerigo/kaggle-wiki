{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v4\n",
    "we basically need a different batch generator. Will first look at what Hooker's got and see if it's worth stealing - an extra thing to think about is to think properly how to deal with the time-dependent and -independent covariates and how best to provide them (particularly for the prediction stage when the dates are 'in the future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from wiki.utils import clock\n",
    "from wiki import rnn, rnn_predict, newphet, val, submissions, rnn_windowed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/'\n",
    "pred_len = 62\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(base_dir+'train_2.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_groups = rnn_windowed.get_page_groups(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(page_groups).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063, 793)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = train_df.drop('Page', axis=1).values ; values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = train_df.columns[1:].values\n",
    "s_date = dates[0]\n",
    "e_date = dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(s_date, e_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ages = np.arange(len(dates))\n",
    "dows = dates.dayofweek.values\n",
    "woys = dates.weekofyear.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expand the dims to make broadcasting work - since numpy\n",
    "#refuses to add dimensions to the right when broadcasting\n",
    "series_idxs = np.arange(values.shape[0])\n",
    "#series_idxs = series_idxs.reshape((series_idxs.shape+(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timedep = np.stack([ages, dows, woys], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesdep = np.array(page_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values, scaler = rnn.scale_values(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the `DataLoaders` aren't gonna work anymore since it makes everything inside a Variable which require gradients. Our embedding indices compute gradient wrt to the embeddings, not the indices, so it breaks. That means we've gotta split it up - so we might as well just do it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class test_datagen(object):\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.generator = self.gen(*args)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.gen(*self.args)\n",
    "    \n",
    "    def gen(self, timeseries, timedep, seriesdep, predlen, batch_size):\n",
    "        \"\"\"\"timeseries: (total, series_length, 1)\n",
    "        timedep: (series_length, num_feats)\n",
    "        seriesdep: (total)\n",
    "        return:\n",
    "               train_series: (batch_size, window_size, 1), \n",
    "            train_timedep: (batch_size, window_size, num_feats),\n",
    "            train_seriesdep: (batch_size, 1)\n",
    "            target_series: (batch_size, window_size, 1)\n",
    "            target_timedep: (batch_size, window_size, num_feats),\n",
    "            target_seriesdep: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        train_series = timeseries[:,:-predlen,:]\n",
    "        target_series = timeseries[:,-predlen:,:]\n",
    "        train_timedep = timedep[:-predlen,:]\n",
    "        target_timedep = timedep[-predlen:,:]\n",
    "        seriesdep = np.expand_dims(seriesdep, axis=-1)\n",
    "        i=0\n",
    "        while i<train_series.shape[0]:\n",
    "            if i+batch_size > train_series.shape[0]:\n",
    "                batch_size = train_series.shape[0] - i\n",
    "\n",
    "            conv = lambda x: torch.from_numpy(x)\n",
    "            yield (\n",
    "                conv(train_series[i:i+batch_size,:,:]).float(),\n",
    "                conv(np.broadcast_to(train_timedep, (batch_size,)+train_timedep.shape)).float(),\n",
    "                conv(seriesdep[i:i+batch_size,:]).long(),\n",
    "                conv(target_series[i:i+batch_size,:,:]).float(),\n",
    "                conv(np.broadcast_to(target_timedep, (batch_size,)+target_timedep.shape)).float(),\n",
    "                conv(seriesdep[i:i+batch_size,:]).long()\n",
    "            )\n",
    "            i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THROWS AWAY LAST EXAMPLES!!! it will break if number of examples is divisible by batch size\n",
    "class train_datagen(object):\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.generator = self.gen(*args)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.gen(*self.args)\n",
    "    \n",
    "    def gen(self, timeseries, timedep, seriesdep, window_size, window_space, num_per_series, batch_size):\n",
    "        \"\"\"\"timeseries: (total, series_length, 1)\n",
    "        timedep: (series_length, num_feats)\n",
    "        seriesdep: (total)\n",
    "        return: (broadcasted if necessary)\n",
    "            train_series: (batch_size, window_size, 1), \n",
    "            train_timedep: (batch_size, window_size, num_feats),\n",
    "            train_seriesdep: (batch_size, 1)\n",
    "            target_series: (batch_size, window_size, 1)\n",
    "            target_timedep: (batch_size, window_size, num_feats),\n",
    "            target_seriesdep: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        train_series, target_series, train_seriesdep = [],[],[]\n",
    "        train_timedep, target_timedep = [],[]\n",
    "        for series, seriesdep in zip(timeseries, seriesdep):\n",
    "            #for k in range(num_per_series):\n",
    "            for k in range(num_per_series):\n",
    "                train_series.append(series[-2*window_size-k*window_space:-window_size-k*window_space, :])\n",
    "                if k != 0:\n",
    "                    target_series.append(series[-window_size-k*window_space:-k*window_space, :])\n",
    "                    target_timedep.append(timedep[-window_size-k*window_space:-k*window_space, :])\n",
    "                else:\n",
    "                    target_series.append(series[-window_size-k*window_space:, :])\n",
    "                    target_timedep.append(timedep[-window_size-k*window_space:, :])\n",
    "                train_timedep.append(timedep[-2*window_size-k*window_space:-window_size-k*window_space, :])\n",
    "                train_seriesdep.append(np.expand_dims(seriesdep, axis=-1))\n",
    "                if len(train_series) == batch_size:\n",
    "                    conv = lambda x: torch.from_numpy(np.stack(x))\n",
    "                    yield (\n",
    "                        conv(train_series).float(),\n",
    "                        conv(train_timedep).float(),\n",
    "                        conv(train_seriesdep).long(),\n",
    "                        conv(target_series).float(),\n",
    "                        conv(target_timedep).float(),\n",
    "                        conv(train_seriesdep).long()\n",
    "                    )\n",
    "                    train_series, target_series, train_seriesdep = [],[],[]\n",
    "                    train_timedep, target_timedep = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traingen = train_datagen(values, timedep, seriesdep, 62, 10, 10, batch_size)\n",
    "valgen = test_datagen(values, timedep, seriesdep, 62, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = rnn_windowed.RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1\n",
      "Running average loss: 0.342636\n",
      "VALIDATION LOSS: 0.356670\n",
      "Elapsed time 436.2991714477539 seconds\n",
      "\n",
      "EPOCH 2\n",
      "Running average loss: 0.342521\n",
      "VALIDATION LOSS: 0.357607\n",
      "Elapsed time 433.38404655456543 seconds\n",
      "\n",
      "EPOCH 3\n",
      "Running average loss: 0.342550\n",
      "VALIDATION LOSS: 0.359487\n",
      "Elapsed time 433.5015790462494 seconds\n",
      "\n",
      "EPOCH 4\n",
      "Running average loss: 0.342500\n",
      "VALIDATION LOSS: 0.358425\n",
      "Elapsed time 432.09529066085815 seconds\n",
      "\n",
      "EPOCH 5\n",
      "Running average loss: 0.342475\n",
      "VALIDATION LOSS: 0.357974\n",
      "Elapsed time 431.5676028728485 seconds\n",
      "\n",
      "EPOCH 6\n",
      "Elapsed time 2295.0635211467743 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e70296b09cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_best_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'rnn_stage2_v4_lr1_weights.mdl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraingen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_best_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p/wiki/wiki/rnn_windowed.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, valloader, optimizer, num_epochs, save_best_path)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m#e is the same for all timesteps so we just pick the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m#one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0membed_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0;31m#Get the time dependent features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "save_best_path = base_dir+'rnn_stage2_v4_lr1_weights.mdl'\n",
    "with clock():\n",
    "    model.fit(traingen, valgen, optimizer=optimizer, num_epochs=25, save_best_path=save_best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_best_path = base_dir+'rnn_stage2_v4_lr1_weights.mdl'\n",
    "model = rnn_meta.RNN().cuda()\n",
    "model.load_state_dict(torch.load(save_best_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "save_best_path = base_dir+'rnn_stage2_v4_lr2_weights.mdl'\n",
    "with clock():\n",
    "    model.fit(trainloader, valloader, optimizer=optimizer, num_epochs=20, save_best_path=save_best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_best_path = base_dir+'rnn_stage2_v4_lr2_weights.mdl'\n",
    "model = rnn_meta.RNN().cuda()\n",
    "model.load_state_dict(torch.load(save_best_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, targets, sequences = model.predict(valgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, predictions = rnn_predict.combine_prediction_data(outputs, targets, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/'\n",
    "train_df = pd.read_csv(base_dir+'train_2.csv')\n",
    "X = train_df.drop('Page', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = scaler.inverse_transform(predictions.T).T\n",
    "true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smapes = val.smape(true[:,-60:], predictions[:,-60:], axis=1)\n",
    "smapes_clipped = val.smape(true[:,-60:], predictions[:,-60:].round().clip(0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nanmean(smapes), np.nanmean(smapes_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(base_dir+'rnn_v3_predictions.npy', outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
