{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import logging as lg\n",
    "lg.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=lg.INFO)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle as pk\n",
    "import glob\n",
    "from fbprophet import Prophet\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import wiki\n",
    "from wiki import utils \n",
    "import multiprocessing as mp\n",
    "total_proc = None\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General column meanings\n",
    "* y_org: what the orginal time series data is (should have missing values staying as missing!)\n",
    "* y: what the model was trained on (the y in val section is kinda irrelevant then)\n",
    "* yhat_org: original unchanged output of the models predict function\n",
    "* yhat: final predictions including post processing\n",
    "# Scoring\n",
    "* Should all be scored using y_org vs yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM: Get new forcasts -> v2.5\n",
    "* y from v2 stays as y because it's what the model was trained on (this should be shown in alphad green?)\n",
    "* Get y from v1.6 into y_org\n",
    "* val line (in red) should be got from y_org\n",
    "* round yhat to nearest int and cast to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2.5\n",
    "NOTE: may not be properly following full column meanings due to y_org being filled with 0s\n",
    "\n",
    "Should set version directory name in next cell. Should describe version specifics (outliers, holidays, validation period)\n",
    "* TRAINING\n",
    "    * Val indexing on -60\n",
    "    * Cut outliers out on upper 95% quartile `forecast.loc[forecast['yhat'] < 0,['yhat']] = 0.0`\n",
    "    * Linear growth\n",
    "    * Fill ALL other NaNs to 0\n",
    "    * Now with try:except: for the `RuntimeError: k initialized to invalid value (-nan)` which replaces first `y` with 0.001\n",
    "    \n",
    "* PREDICTIONS\n",
    "    * Truncating predictions at 0 \n",
    "    * Rounding to nearest int\n",
    "\n",
    "\t* Need this now as the outlier cutting might remove the initial 0.001 we put in there\n",
    "### Remarks\n",
    "* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROPHET_PATH = '../data/prophet/'\n",
    "RESULTS_PATH = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TARGET_VERSION = 'v2.5/'\n",
    "BASE_VERSION = 'v2/'\n",
    "Y_VERSION = 'v1.6/'\n",
    "#os.makedirs(PROPHET_PATH+TARGET_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_forecast_files = [x.split('/')[-1] for x in glob.glob(PROPHET_PATH+BASE_VERSION+'*df.f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "previously_processed_files = [x.split('/')[-1] for x in glob.glob(PROPHET_PATH+TARGET_VERSION+'*df.f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_files = full_forecast_files# list(set(full_forecast_files) - set(previously_processed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61780df.f', '88262df.f']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['61780', '88262']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[:-4] for x in forecast_files[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2df = pd.read_feather(PROPHET_PATH+BASE_VERSION+forecast_files[0])\n",
    "v16df = pd.read_feather(PROPHET_PATH+Y_VERSION+forecast_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v25df = pd.read_feather(PROPHET_PATH+TARGET_VERSION+'61780df.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145063/145063 [27:56<00:00, 86.53it/s]\n"
     ]
    }
   ],
   "source": [
    "val_results = []\n",
    "for file in tqdm(forecast_files):\n",
    "    v2df = pd.read_feather(PROPHET_PATH+BASE_VERSION+file)\n",
    "    v16df = pd.read_feather(PROPHET_PATH+Y_VERSION+file)\n",
    "    # y_org to be the orginal timeseries data\n",
    "    #v2df.loc[:,'y_org'] = v16df.y\n",
    "    # y is already set correcly, as the training data\n",
    "    # yhat_org is already set correctly (in the creating process)\n",
    "    # round and cast yhat to nearest int\n",
    "    v2df.loc[:,'yhat'] = v2df.yhat.round(0).astype(int)\n",
    "    #v2df.to_feather(PROPHET_PATH+TARGET_VERSION+file)\n",
    "    full_smape = wiki.val.smape(v16df.y, v2df.yhat)\n",
    "    val_smape = wiki.val.smape(v16df[v16df['train'] == 0].y,v2df[v2df['train'] == 0].yhat)\n",
    "    val_results.append((file[:-4], full_smape, val_smape))\n",
    "val_results = pd.DataFrame(val_results, columns=['page_index',TARGET_VERSION[:-1]+'_full',TARGET_VERSION[:-1]+'_val'])\n",
    "val_results.to_feather(PROPHET_PATH+RESULTS_PATH+TARGET_VERSION[:-1]+'df.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/142346 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 3/142346 [00:00<1:35:05, 24.95it/s]\u001b[A\n",
      "  0%|          | 5/142346 [00:00<1:45:40, 22.45it/s]\u001b[A\n",
      "  0%|          | 8/142346 [00:00<1:47:17, 22.11it/s]\u001b[A\n",
      "  0%|          | 10/142346 [00:00<2:01:57, 19.45it/s]\u001b[A\n",
      "  0%|          | 12/142346 [00:00<2:10:56, 18.12it/s]\u001b[A\n",
      "  0%|          | 14/142346 [00:00<2:08:32, 18.45it/s]\u001b[A\n",
      "  0%|          | 17/142346 [00:00<1:58:57, 19.94it/s]\u001b[A\n",
      "  0%|          | 20/142346 [00:00<1:56:20, 20.39it/s]\u001b[A\n",
      "  0%|          | 22/142346 [00:01<2:02:06, 19.43it/s]\u001b[A\n",
      "  0%|          | 24/142346 [00:01<2:09:48, 18.27it/s]\u001b[A\n",
      "  0%|          | 27/142346 [00:01<2:02:41, 19.33it/s]\u001b[A\n",
      "  0%|          | 29/142346 [00:01<2:05:11, 18.95it/s]\u001b[A\n",
      "  0%|          | 31/142346 [00:01<2:12:17, 17.93it/s]\u001b[A\n",
      "  0%|          | 33/142346 [00:01<2:14:50, 17.59it/s]\u001b[A\n",
      "  0%|          | 35/142346 [00:01<2:14:08, 17.68it/s]\u001b[A\n",
      "  0%|          | 37/142346 [00:01<2:21:16, 16.79it/s]\u001b[A\n",
      "  0%|          | 39/142346 [00:02<2:24:06, 16.46it/s]\u001b[A\n",
      "  0%|          | 41/142346 [00:02<2:18:34, 17.11it/s]\u001b[A\n",
      "  0%|          | 43/142346 [00:02<2:22:54, 16.60it/s]\u001b[A\n",
      "  0%|          | 45/142346 [00:02<2:19:53, 16.95it/s]\u001b[A\n",
      "  0%|          | 47/142346 [00:02<2:15:50, 17.46it/s]\u001b[A\n",
      "  0%|          | 49/142346 [00:02<2:25:26, 16.31it/s]\u001b[A\n",
      "  0%|          | 52/142346 [00:02<2:10:04, 18.23it/s]\u001b[A\n",
      "  0%|          | 55/142346 [00:02<2:07:35, 18.59it/s]\u001b[A\n",
      "  0%|          | 58/142346 [00:03<2:02:32, 19.35it/s]\u001b[A\n",
      "  0%|          | 60/142346 [00:03<2:09:12, 18.35it/s]\u001b[A\n",
      "  0%|          | 63/142346 [00:03<2:03:10, 19.25it/s]\u001b[A\n",
      "  0%|          | 65/142346 [00:03<2:04:24, 19.06it/s]\u001b[A\n",
      "  0%|          | 68/142346 [00:03<1:58:12, 20.06it/s]\u001b[A\n",
      "  0%|          | 71/142346 [00:03<1:55:26, 20.54it/s]\u001b[A\n",
      "  0%|          | 74/142346 [00:03<1:58:42, 19.97it/s]\u001b[A\n",
      "  0%|          | 77/142346 [00:04<2:02:23, 19.37it/s]\u001b[A\n",
      "100%|██████████| 142346/142346 [1:47:51<00:00, 22.00it/s] \n"
     ]
    }
   ],
   "source": [
    "val_results = []\n",
    "for file in tqdm(forecast_files):\n",
    "    v2df = pd.read_feather(PROPHET_PATH+BASE_VERSION+file)\n",
    "    v16df = pd.read_feather(PROPHET_PATH+Y_VERSION+file)\n",
    "    # y_org to be the orginal timeseries data\n",
    "    v2df.loc[:,'y_org'] = v16df.y\n",
    "    # y is already set correcly, as the training data\n",
    "    # yhat_org is already set correctly (in the creating process)\n",
    "    # round and cast yhat to nearest int\n",
    "    v2df.loc[:,'yhat'] = v2df.yhat.round(0).astype(int)\n",
    "    v2df.to_feather(PROPHET_PATH+TARGET_VERSION+file)\n",
    "    full_smape = wiki.val.smape(v2df.y_org, v2df.yhat)\n",
    "    val_smape = wiki.val.smape(v2df[v2df['train'] == 0].y_org,v2df[v2df['train'] == 0].yhat)\n",
    "    val_results.append((file[:-4], full_smape, val_smape))\n",
    "val_results = pd.DataFrame(val_results, columns=['page_index',TARGET_VERSION[:-1]+'_full',TARGET_VERSION[:-1]+'_val'])\n",
    "val_results.to_feather(PROPHET_PATH+RESULTS_PATH+TARGET_VERSION[:-1]+'df.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsdf = pd.read_feather(PROPHET_PATH+RESULTS_PATH+TARGET_VERSION[:-1]+'df.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.497131263774811"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsdf['v2.5_val'].mean()\n",
    "# previous best was v2 at 58.81101798102403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
